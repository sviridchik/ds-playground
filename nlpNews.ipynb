{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split,KFold,GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: to find open dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/victoria/.local/lib/python3.8/site-packages (1.22.3)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/victoria/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/victoria/.local/lib/python3.8/site-packages (from nltk) (2022.6.2)\n",
      "Requirement already satisfied: joblib in /home/victoria/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /home/victoria/.local/lib/python3.8/site-packages (from nltk) (4.64.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/victoria/.local/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/victoria/.local/lib/python3.8/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/victoria/.local/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/victoria/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/victoria/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/victoria/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/victoria/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/train.tsv\", sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_fake.value_counts()\n",
    "data[\"len\"] = data[\"title\"].map(lambda el: len(el))\n",
    "print(\"len of titles\", data[\"len\"].min(), data[\"len\"].max())\n",
    "# balance\n",
    "print(data[\"len\"].value_counts())\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data[\"title\"] = data[\"title\"].map(lambda el: el.lower())\n",
    "data[\"title\"] = data[\"title\"].replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tokens\"] = data[\"title\"].map(lambda el: nltk.word_tokenize(el, language=\"russian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatization and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ru = stopwords.words('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_proccessing():\n",
    "    cv = TfidfVectorizer(min_df=1)\n",
    "    XX = cv.fit_transform(data[\"tokens\"])\n",
    "    # print(X,X.shape,\"\\n\",len(cv.get_feature_names_out()))\n",
    "    # print(data[\"tfidf\"].shape,len(cv.get_feature_names()))\n",
    "    # print(cv.get_feature_names_out())\n",
    "    X = pd.DataFrame(data=XX.toarray(), columns=cv.get_feature_names_out())\n",
    "    y = data[\"is_fake\"]\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bag_of_words_proccessing():\n",
    "    counter = Counter()\n",
    "\n",
    "    for sample in data[\"tokens\"]:\n",
    "        for word in sample.split():\n",
    "            counter[word] += 1\n",
    "\n",
    "    columns = [word for word, _ in counter.most_common(k)]\n",
    "\n",
    "    def sentence_to_vec(el):\n",
    "        vec = np.zeros(k)\n",
    "        for word in el.split():\n",
    "            if word in columns:\n",
    "                vec[columns.index(word)] += 1\n",
    "        return vec\n",
    "\n",
    "    data[\"data\"] = data[\"tokens\"].map(lambda el: sentence_to_vec(el))\n",
    "    # print(data[\"data\"].head())\n",
    "    X = pd.DataFrame(data=list(data[\"data\"]), columns=columns)\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_f(X):\n",
    "    y = data[\"is_fake\"]\n",
    "#     X = tfidf_proccessing()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    # check\n",
    "    # print(y_train.value_counts())\n",
    "    # print(y_test.value_counts())\n",
    "    model = LogisticRegression()\n",
    "    # model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    # print(f1_score(y_test, y_predict))\n",
    "    return f1_score(y_test, y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_dependence = []\n",
    "for k in range(500, 2000, 100):\n",
    "    X = bag_of_words_proccessing()\n",
    "    # X = tfidf_proccessing()\n",
    "    # y = data[\"is_fake\"]\n",
    "\n",
    "    # SCALER\n",
    "    #     scaler = StandardScaler()\n",
    "    #     X  = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "    f = get_f(X)\n",
    "    print(f)\n",
    "    k_dependence.append((k, f))\n",
    "k_dependence.sort(key=lambda el: el[-1])\n",
    "print(k_dependence)\n",
    "\n",
    "a, b = map(list, zip(*k_dependence))\n",
    "plt.plot(a,b)\n",
    "plt.show()\n",
    "\n",
    "for k in range(1700, 1900, 50):\n",
    "    X = bag_of_words_proccessing()\n",
    "    # SCALER\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X = scaler.fit_transform(X)\n",
    "    f = get_f(X)\n",
    "    k_dependence.append((k, f))\n",
    "print(k_dependence)\n",
    "k_dependence.sort(key=lambda el: el[-1])\n",
    "print(f\"k the best is {k_dependence[-1][0]}\")\n",
    "#     !!! k не влияет на tf idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grid():\n",
    "    print(LogisticRegression().get_params().keys())\n",
    "    y = data[\"is_fake\"]\n",
    "    X = tfidf_proccessing()\n",
    "    param_grid = {\n",
    "        'random_state':[42],\n",
    "        'penalty':['l1','l2'],\n",
    "        'C': np.logspace(-4,4,10),\n",
    "        'solver': ['liblinear']\n",
    "    }\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    kf = KFold(n_splits= 5,random_state=42, shuffle=True)\n",
    "    model = LogisticRegression()\n",
    "    gs = GridSearchCV(model,cv=5, param_grid=param_grid, scoring=\"f1\")\n",
    "    gs.fit(X_train,y_train)\n",
    "    print(gs.best_params_)\n",
    "    print(gs)\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_estimator_)\n",
    "    # y_predict = gs.best_estimator_.predict(X,y)\n",
    "    # print(f1_score(y_test, y_predict))\n",
    "grid()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
